Data-Analysis
All the analysis in this repository are done using Python programing language and SQL

This repository is on data analysis. It is making use of python and the libraries in python like  pandas, pandas_profiling, 
numpy, sklearn, seaborn, matplotlib, Wordclouds and dtreeviz to carry out the Exploratory Data Analysis (EDA) and 
to also predict through regressional analysis.
All the codes are written in python and the interest is to show how to analyse both numeric and word data. 
The codes analyze, identify patterns, nd explores the data. They then model, predict, solves the problem, and 
presents the visual, report, and problem-solving solution.


The codes that have been added are:
1. The file "fake-news.ipynb" is for Exploratory Data Analysis (EDA) of data of news published on different sites, identifies patterns
2. The file "insurrance-project.ipynb" is for Exploratory Data Analysis (EDA) of insurance company expenses on their clients based on their behavior and health challenges.
3. The file "titanic-logical-regression.ipynb" is for Exploratory Data Analysis (EDA) to relate the survival of the passenger in titanic to different conditions like age, class they belong, amount paid and number of dependents
4. The file "Database-creation-and-analysis-using-MySQL.ipynb" is making use of SQL through MySQL to create database, retreive record, update record, and formatting list records
5. The "Machine-Learning-on-google-cloud-dataset-using-BigQuery.ipynb" makes use of Biquery and SQL to access dataset in google cloud and build machine learning for the train set
6. The file "Comprehensive-analysis-google-cloud-dataset-using-SQL-and-BigQuery" makes use of the Bigqueryhelper and SQL to access data from the database and process it. It also include comprehensive analysis and mining of the records from the database



Some of the library used include pandas, pandas_profiling, numpy, sklearn, seaborn, matplotlib, dtreeviz.

In summary, the following steps were taking in the data analysis:
i). Read In the data
ii). Examine the data and Cleaning Data
iii). Explore the Data
iv). Data Analysis
v). Data Visualization
vi). Choosing the Best predictive Model
